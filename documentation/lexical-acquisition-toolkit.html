<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="shortcut icon" href="images/favicon.ico">
    <meta charset="utf-8">
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
<meta name="apple-mobile-web-app-status-bar-style" content="black" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
<meta name="apple-mobile-web-app-status-bar-style" content="black" />

    <link rel="stylesheet" href="/css/bootstrap.css">
<link rel="stylesheet" href="/css/flat-ui.css">
<link rel="stylesheet" href="/css/icon-font.css">
<link rel="stylesheet" href="/css/animations.css">
<link rel="stylesheet" href="/css/style.css">

    <title>Lexical acquisition toolkit</title>
  </head>
  <body data-spy="scroll" data-target=".bs-sidebar" data-offset="90">
    <header class="header-2">
<div class="container">
  <div class="row">
    <nav class="navbar col-sm-12" role="navigation">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle"></button>
      <a class="brand" href="/"><img src="/images/opener_log.png" width="125" height="58" alt=""> </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav pull-right">
        <li class="active"><a href="/index.html">Home</a></li>
<li><a href="/getting-started">Getting started</a></li>
<li><a href="/documentation">Documentation</a></li>
<li><a href="/webservices">Webservices</a></li>
<li><a href="/project">The Project</a></li>
<li><a href="/support.html">Support</a></li>
<li><a href="https://github.com/opener-project">GitHub</a></li>


      </ul>
      <ul class="subnav">
        <li><a href="#">Privacy</a></li>
        <li><a href="#">Terms</a></li>
        <li><a href="#">About</a></li>
        <li><a href="#">Contact Us</a></li>
      </ul>
    </div>
    </nav>
  </div>
</div>
</header>


    <div class="header-2-sub bg-midnight-blue low">
      <div class="background">&nbsp;</div>
      <div class="container">
        <div class="row">
          <div class="col-sm-8 col-sm-offset-2"> </div>
        </div>
      </div>
    </div>

    <div class="container bs-docs-container">
      <div class="row">
        <div class="col-md-3">
          <!--Nav Bar -->
          <div class="bs-sidebar hidden-print" role="complementary" data-spy="affix" data-offset-top="60">
            
            <ul id="sidebar" class="nav bs-sidenav">
  <li>
    <a href="#readme">Lexical acquisition toolkit</a>
    <ul class="nav nav-stacked visible">
      <li><a href="#readme">Introduction</a></li>
      <li><a href="#reference">Reference</a></li>
      <li><a href="http://opener.olery.com/lexical-acquisition-toolkit">Webservice</a></li>
      <li><a href="https://github.com/opener-project/opinion-domain-lexicon-acquisition">View Source code</a></li>
    </ul>
  </li>
  <li><a href="/documentation/">Other Components</a></li>
</ul>


          </div>
        </div>
        <!--Main Content -->
        <div class="col-sm-8 col-sm-offset-1 content">
          <div id="readme"></div>

<h1 id="acquisition-of-expressions-and-targets-lexicons-from-a-domain">Acquisition of expressions and targets lexicons from a domain</h1>

<p>This toolkit allows to generate domain specific lexicons:<br />
* Polarity or expression lexicons: lexicons with words in the specific domain used to state opinions<br />
* Target or property lexicons: lexicons with expressions that represent properties of the entities<br />
represented in the given domain (for a hotel review domain, these properties could be the rooms, the staff<br />
or the ambience). </p>

<p>Usually the polarity expressions are used to give opinions about properties.</p>

<p>Two approaches for automatically generating these domains have been implemented<br />
and are available within this toolkit:<br />
* Supervised acquisition: from domain annotated data<br />
* Unsupervised acquisition: from raw data belonging to the domain</p>

<h2 id="supervised-acquisition">Supervised Acquisition</h2>

<p>This approach is implemented in the script <code>acquire_from_annotated_data.py</code>. It basically takes as input<br />
a set of KAF/NAF files annotated with opinions (targets, holders and expressions), and generates 2 CSV<br />
output files with the most relevant expressions and targets. You can call directly to the script with<br />
the option -h or –help to see the parameters:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ acquire_from_annotated_data.py -h
usage: acquire_from_annotated_data.py [-h] (-l file_with_list | -f folder)
                                      -exp_csv expressions_file.csv -tar_csv
                                      targets_file.csv

Extract expressions and targets from annotated data

optional arguments:
  -h, --help            show this help message and exit
  -l file_with_list, --list file_with_list
                        A file with a list of paths to KAF/NAF files
  -f folder, --folder folder
                        A folder with KAF/NAF files
  -exp_csv expressions_file.csv
                        CSV file to store the expressions
  -tar_csv targets_file.csv
                        CSV file to store the targets

</pre></div>
</div>
</div>

<p>The input KAF/NAF files can be specified using in 2 exclusive ways:<br />
* Providing a file which contains a list of paths to the annotated files (one per line) (option -l / –list)<br />
* Providing a folder which contains the annotated files, all files with extension .kaf or .naf will be processed<br />
(option -f / –folder)</p>

<p>There are two mandatory output parameters to specify where the expression and targets lexicons must be stored (the<br />
options -exp_cvs and -tar_csv. </p>

<p>The script also prints the lexicons on the standard output in a more user readable way, as well as some debugging<br />
information on the error output. One example of usage of this program would be:</p>

<div><div class="CodeRay">
  <div class="code"><pre>acquire_from_annotated_data.py -f ~/data/hotel -exp_csv my_expressions.csv -tar_csv my_targets.csv &gt; log.out 2&gt; log.err
</pre></div>
</div>
</div>

<p>This would read all the KAF/NAF files in the folder ~/data/hole and store the output in the file log.out, the debugging information in the file log.err, and the resulting<br />
lexicons in CSV format on the files my_expressions.csv and my_targets.csv respectively.</p>

<h2 id="unsupervised-acquisition">Unsupervised Acquisition</h2>

<p>With this approach it is possible to build an expression and a target lexicon from raw data from the domain. The method is based on patterns and frequency distribution of words in our new domain,<br />
so usually the larger our domain corpus is, the best lexicon we will get. The process is divided into 3 steps:<br />
* Data preparation<br />
* Creation of the indexes<br />
* Running the lexicon creator</p>

<h3 id="data-preparation">Data preparation</h3>

<p>We need first to annotate all the data that we have with this information:<br />
* Token and sentence information: which is provided by our tokenisers (https://github.com/opener-project/tokenizer)<br />
* Lemma and part-of-speech information: which is provided by our pos-taggers (https://github.com/opener-project/pos-tagger https://github.com/opener-project/opener-tree-tagger)</p>

<p>So you will need to tag all your plain files with the required information using these 2 tools and store all the resulting KAF files within a folder (is not necessary but it is the easiest)</p>

<h3 id="creation-of-the-indexes">Creation of the indexes</h3>

<p>This tool reads all the KAF files created in the previous step and generates several indexes with ngram frequencies and extra information that will be used in the last step when creating the lexicons.<br />
This step needs to be performed just once, so when you create the indexes from your KAF files, you can reuse these indexes as many times as you want, and they implement a cache optimisation for making<br />
the queries more efficiently. The script which implements this is “create_index.py”. It accepts a list of parameters to customize the indexes, you can check the accepted parameters by calling to the script<br />
with the option -h:</p>

<div><div class="CodeRay">
  <div class="code"><pre>create_index.py -h
usage: create_index.py [-h]
                       (-input_file file_with_list | -input_folder folder)
                       -output folder
                       [-punc punctuation_symbols default &quot;.?;!&quot;)] [-no_lower]
                       [-max_ngram integer (default 3)]
                       [-min_ngram integer (default 1)] [-no_sent_borders]
                       [-no_remove_out] [-min_freq integer (default 1]

Create n-gram indexes from a set of KAF/NAF files

optional arguments:
  -h, --help            show this help message and exit
  -punc punctuation_symbols (default &quot;.?;!&quot;)
                        Symbols to be considered as punctuation
  -no_lower             Tokens are NOT converted to lowercase (by default they
                        are)
  -max_ngram integer (default 3)
                        Maximum size of n-grams to create
  -min_ngram integer (default 1)
                        Mininum size of n-grams to create
  -no_sent_borders      Sentence delimiters are NOT included (by default they
                        are)
  -no_remove_out        Output folder is not removed if exists
  -min_freq integer (default 1)
                        Minimum frequency allowed for ngram

Required arguments:
  -input_file file_with_list
                        File with a list of paths to KAF/NAF files, one per
                        line
  -input_folder folder  Folder with a set of KAF/NAF files
  -output folder        Output folder to store the indexes
</pre></div>
</div>
</div>

<p>The only required arguments are those to specify the input files (via a folder or via a file with a list of paths to your KAF files), and the name of the output folder where you want to store the new indexes. The rest<br />
of parameters are used to specify some characteristics of your ngram indexes, like the maximum length of the ngram (-max_ngram), the minimum frequency allowed for an ngram in your index (-min_freq). So considering<br />
you have all your KAF (with tokens and terms) in a folder called my_kaf_files, you want to create indexes for ngram with length up to 3 and with minimum frequency of 2, and you want to store the indexes in my_indexes, you<br />
should run this command:
<code>shell
create_index.py -input_folder my_kaf_files -max_ngram 3 -min_freq 2 -output my_indexes
</code></p>

<p>Once you have created this index you can reuse it as many times as you want. It has implemented an internal cache for speeding up queries already done, so the first time you use the indexes the speed could be low if you<br />
are using large datasets or creating indexes for long ngrams (usually in these cases by forcing each ngram to appear at least twice with the option -min_freq 2 for the create_index.py).</p>

<h3 id="lexicon-creator">Lexicon creator</h3>

<p>Once you have created the index of ngram frequencies, you can create the lexicons. You will have to provide with a small amount of information to represent your domain. We provide in the toolkit with some example<br />
files that you can use as an start or modify to suit your needs. These are the three pieces of information required:<br />
* A list of polarity seeds along with the polarity of those words in your domain<br />
* A list of patterns that represent the relation between polarity words and targets/aspects in your domain<br />
* A list of patterns that represent usages of polarity words with the same/opposite polarity</p>

<p>This is an example the polarity seed file, where there is a polarity word per line and a symbol + or - depending on the polarity of this word:</p>

<div><div class="CodeRay">
  <div class="code"><pre>good +
bad -
nice +
ugly -
</pre></div>
</div>
</div>

<p>This is an example of the relational patterns. Each line contains a pattern with the placeholders [EXP] representing the polarity word and [TAR] standing for the target or aspect
<code>shell
a [EXP] [TAR]
the [EXP] [TAR]
</code></p>

<p>And finally below you can see and example of the patterns that represents usages of words with the same polarity. There is one pattern per line, with the placeholders [A] and [B] representing the polarity words<br />
, and the firs symbol of the line can be either = or ! indicating that the pattern represents an example of words with the same polarity (=) or with opposite polarities (!)
<code>shell
= [A] and [B]
= [A] and very [B]
! [A] but [B]
</code></p>

<p>As already said, in the subfolder <code>resources</code> you can find some examples of these files, that you could you straight forward or adapt them.</p>

<p>Once you have these three list ready and the index created, you can run the lexicon creator. The script is <code>acquire_from_raw_data.py</code>. The usage and possible parameteres are shown by calling to the script with the -h argument.<br />
````shell<br />
acquire_from_raw_data.py -h<br />
usage: acquire_from_raw_data.py [-h] -index index_folder -seeds<br />
                                file_with_seeds -patterns file_with_patterns<br />
                                -p_pol file_with_patterns [-lang, lang_code]<br />
                                [-no_verbose] [-min_freq integer]<br />
                                [-target_pos list of tags “N R G”]<br />
                                [-expression_pos list of tags “N R G”]<br />
                                [-max_iter integer]</p>

<p>Creates expression and target lexicons from domain raw data</p>

<p>optional arguments:<br />
  -h, –help            show this help message and exit<br />
  -lang, lang_code, -l lang_code<br />
                        Force to use this lang, otherwise the language from<br />
                        the indexs is used<br />
  -no_verbose           No verbose log information<br />
  -min_freq integer, -mf integer<br />
                        Minimum frequency allowed for a query (default 1)<br />
  -target_pos list of tags “N R G”<br />
                        Allowed pos tags for targets (default “N” nouns) Use<br />
                        ALL for all possible pos<br />
  -expression_pos list of tags “N R G”<br />
                        Allowed pos tags for expressions (default “G”<br />
                        adjectives) Use ALL for all possible pos<br />
  -max_iter integer     Maximum number of iterations (default 5)</p>

<p>Required arguments:<br />
  -index index_folder   Folder with ngram indexes<br />
  -seeds file_with_seeds, -s file_with_seeds<br />
                        File with seeds, one per line<br />
  -patterns file_with_patterns, -p file_with_patterns<br />
                        File with patterns, one per line (example-&gt; “a [Exp]<br />
                        [Tar]”)<br />
  -p_pol file_with_patterns<br />
                        File with patterns for guessing the polarity, one per<br />
                        line (example-&gt; “# [A] and [B]”)<br />
````</p>

<p>The required parameters are the folder where we stored our indexes and the three lists of seeds and patterns. The rest of parameters are optional, and the values<br />
used by default if they are not specified can be seen in the help output of the script. If you want to create the lexicons using the indexes stored in my_indexes<br />
and the examples files provided for English, you should run:
<code>shell
acquire_from_raw_data.py -index my_indexes -seeds resources/seeds/en.txt -patterns resources/patterns/en.txt -p_pol resources/patterns_guess_polarity/en.txt
</code></p>

<h2 id="installation">Installation</h2>
<p>The only requirement of this toolkit is to have installed the KafNafParserPy, which can be found at https://github.com/opener-project/KafNafParserPy<br />
Then you need only to make sure the KafNafParserPy library is on the Python path and clone this repository.</p>

<h2 id="contact">Contact</h2>
<ul>
  <li>Ruben Izquierdo</li>
  <li>Vrije University of Amsterdam</li>
  <li>ruben.izquierdobevia@vu.nl</li>
</ul>

        </div>
      </div>
    </div>

    <footer class="footer-2 bg-midnight-blue doc-bottom-margin">
<div class="container">
  <nav class="pull-left">
  <ul>
    <li class="active"><a href="/index.html">Home</a></li>
<li><a href="/getting-started">Getting started</a></li>
<li><a href="/documentation">Documentation</a></li>
<li><a href="/webservices">Webservices</a></li>
<li><a href="/project">The Project</a></li>
<li><a href="/support.html">Support</a></li>
<li><a href="https://github.com/opener-project">GitHub</a></li>


  </ul>
  </nav>
  <!--<div class="social-btns pull-right">-->
    <!--<a href="#">-->
      <!--<div class="fui-vimeo"></div>-->
      <!--<div class="fui-vimeo"></div>-->
    <!--</a>-->
    <!--<a href="#">-->
      <!--<div class="fui-facebook"></div>-->
      <!--<div class="fui-facebook"></div>-->
    <!--</a>-->
    <!--<a href="#">-->
      <!--<div class="fui-twitter"></div>-->
      <!--<div class="fui-twitter"></div>-->
    <!--</a>-->
  <!--</div>-->
  <!--<div class="additional-links">-->
    <!--Be sure to take a look at our <a href="#">Terms of Use</a> and <a href="#">Privacy Policy</a>-->
  <!--</div>-->
</div>
</footer>
<div class=container">
  <div class="eu-notice">
    <img src="/images/flag_yellow_low.jpg" height="34" width="50"/> This project has received funding from the European Union’s Seventh
    Framework Programme for research, technological development and
    demonstration under grant agreement no 261712.
  </div>
</div>

    <!-- Placed at the end of the document so the pages load faster -->
    <script src="/js/jquery-2.1.0.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/modernizr.custom.js"></script>
<script src="/js/page-transitions.js"></script>
<script src="/js/easing.min.js"></script>
<script src="/js/jquery.svg.js"></script>
<script src="/js/jquery.svganim.js"></script>
<script src="/js/jquery.backgroundvideo.min.js"></script>
<script src="/js/froogaloop.min.js"></script>
<script src="/js/startup-kit.js"></script>

  </body>
</html>
